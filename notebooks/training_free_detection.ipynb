{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2XC2kNYrgBaY"
      },
      "outputs": [],
      "source": [
        "# # %% [setup]\n",
        "# import sys, subprocess\n",
        "\n",
        "# def _pip(pkg):\n",
        "#     try:\n",
        "#         __import__(pkg)\n",
        "#     except Exception:\n",
        "#         subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg])\n",
        "\n",
        "# for p in ['torch', 'torchvision', 'matplotlib', 'numpy', 'Pillow']:\n",
        "#     _pip(p)\n",
        "\n",
        "# !git clone https://github.com/facebookresearch/dinov3.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Po5tuIe8fFCj",
        "outputId": "5537737e-16c0-42cb-9cc2-9652814e30f3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "sys.path.append(\"../\")\n",
        "import torch\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.widgets import Button\n",
        "from utils.download_image import download_image_from_url"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DOWNLOAD FROM URL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dcXOGa0B-LWv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloaded and saved image as: downloads/998x0_1.jpg\n",
            "Using image file at: downloads/998x0_1.jpg\n"
          ]
        }
      ],
      "source": [
        "# https://s.hdnux.com/photos/01/36/56/03/24830199/1/998x0.jpg\n",
        "query_image_pre_bbox = download_image_from_url()\n",
        "if query_image_pre_bbox and os.path.exists(query_image_pre_bbox):\n",
        "    print(f\"Using image file at: {query_image_pre_bbox}\")\n",
        "else:\n",
        "    print(\"No valid image file is available.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SELECT BBOX and CROP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%matplotlib widget\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Rectangle\n",
        "from matplotlib.widgets import Button\n",
        "from PIL import Image\n",
        "\n",
        "# --- Caricamento immagine dal path nella variabile richiesta ---\n",
        "try:\n",
        "    query_image_pre_bbox  # noqa: F821\n",
        "except NameError as e:\n",
        "    raise NameError(\"Definisci prima `query_image_pre_bbox = 'path/alla/immagine.jpg'`.\") from e\n",
        "\n",
        "if not isinstance(query_image_pre_bbox, str):\n",
        "    raise TypeError(\"`query_image_pre_bbox` deve essere una STRINGA col percorso dell'immagine.\")\n",
        "if not os.path.exists(query_image_pre_bbox):\n",
        "    raise FileNotFoundError(f\"File non trovato: {query_image_pre_bbox}\")\n",
        "\n",
        "_img_pil = Image.open(query_image_pre_bbox).convert(\"RGB\")\n",
        "img = np.array(_img_pil)  # usiamo 'img' come nel tuo esempio\n",
        "\n",
        "# --- Stato interazione (una sola box) ---\n",
        "start_pt = None          # (x0, y0) primo click\n",
        "rect_patch = None        # patch Rectangle mostrata a schermo\n",
        "current_box = None       # (x0, y0, x1, y1) finale\n",
        "cid_click = None\n",
        "cid_move = None\n",
        "\n",
        "def clamp(val, lo, hi):\n",
        "    return max(lo, min(val, hi))\n",
        "\n",
        "def onclick(event):\n",
        "    global start_pt, rect_patch, current_box, cid_move\n",
        "    if event.inaxes != ax_img:\n",
        "        return\n",
        "    if event.xdata is None or event.ydata is None:\n",
        "        return\n",
        "\n",
        "    x, y = int(event.xdata), int(event.ydata)\n",
        "\n",
        "    # 1° click: inizia box + attiva \"rubberband\" col mouse move\n",
        "    if start_pt is None:\n",
        "        start_pt = (x, y)\n",
        "        # crea o resetta la patch\n",
        "        if rect_patch is None:\n",
        "            rect_patch = Rectangle((x, y), 1, 1, fill=False, linewidth=2)\n",
        "            ax_img.add_patch(rect_patch)\n",
        "        else:\n",
        "            rect_patch.set_xy((x, y))\n",
        "            rect_patch.set_width(1)\n",
        "            rect_patch.set_height(1)\n",
        "            rect_patch.set_visible(True)\n",
        "        # collega movimento per aggiornare dimensioni\n",
        "        connect_motion()\n",
        "        fig.canvas.draw_idle()\n",
        "    # 2° click: fissa box e disconnette il movimento\n",
        "    else:\n",
        "        x0, y0 = start_pt\n",
        "        x1, y1 = x, y\n",
        "        # normalizza in [min,max]\n",
        "        x0, x1 = sorted([x0, x1])\n",
        "        y0, y1 = sorted([y0, y1])\n",
        "\n",
        "        H, W = img.shape[:2]\n",
        "        x0 = clamp(x0, 0, W-1); x1 = clamp(x1, 0, W-1)\n",
        "        y0 = clamp(y0, 0, H-1); y1 = clamp(y1, 0, H-1)\n",
        "\n",
        "        # aggiorna patch finale\n",
        "        rect_patch.set_xy((x0, y0))\n",
        "        rect_patch.set_width(max(1, x1 - x0))\n",
        "        rect_patch.set_height(max(1, y1 - y0))\n",
        "        fig.canvas.draw_idle()\n",
        "\n",
        "        current_box = (x0, y0, x1, y1)\n",
        "        start_pt = None\n",
        "        disconnect_motion()\n",
        "\n",
        "def onmove(event):\n",
        "    # aggiorna \"rubberband\" durante il drag\n",
        "    if start_pt is None or event.inaxes != ax_img:\n",
        "        return\n",
        "    if event.xdata is None or event.ydata is None:\n",
        "        return\n",
        "    x0, y0 = start_pt\n",
        "    x1, y1 = int(event.xdata), int(event.ydata)\n",
        "\n",
        "    # calcola box parziale\n",
        "    xx0, xx1 = sorted([x0, x1])\n",
        "    yy0, yy1 = sorted([y0, y1])\n",
        "\n",
        "    H, W = img.shape[:2]\n",
        "    xx0 = clamp(xx0, 0, W-1); xx1 = clamp(xx1, 0, W-1)\n",
        "    yy0 = clamp(yy0, 0, H-1); yy1 = clamp(yy1, 0, H-1)\n",
        "\n",
        "    if rect_patch is None:\n",
        "        return\n",
        "    rect_patch.set_xy((xx0, yy0))\n",
        "    rect_patch.set_width(max(1, xx1 - xx0))\n",
        "    rect_patch.set_height(max(1, yy1 - yy0))\n",
        "    fig.canvas.draw_idle()\n",
        "\n",
        "def connect_motion():\n",
        "    global cid_move\n",
        "    if cid_move is None:\n",
        "        cid_move = fig.canvas.mpl_connect('motion_notify_event', onmove)\n",
        "\n",
        "def disconnect_motion():\n",
        "    global cid_move\n",
        "    if cid_move is not None:\n",
        "        fig.canvas.mpl_disconnect(cid_move)\n",
        "        cid_move = None\n",
        "\n",
        "def on_ok(event):\n",
        "    \"\"\"Esegue il crop dalla box selezionata e chiude la figura principale.\"\"\"\n",
        "    global query_image\n",
        "    if current_box is None:\n",
        "        print(\"Nessuna box definita: fai due click (inizio/fine) sull'immagine.\")\n",
        "        return\n",
        "    x0, y0, x1, y1 = current_box\n",
        "    if x1 <= x0 or y1 <= y0:\n",
        "        print(\"Box non valida. Ridisegna la box.\")\n",
        "        return\n",
        "\n",
        "    # Crop (usiamo slicing su numpy; img è RGB)\n",
        "    query_image = img[y0:y1, x0:x1].copy()\n",
        "\n",
        "    # Mostra il crop in una nuova finestra\n",
        "    fig2, ax2 = plt.subplots(1, 1, figsize=(6, 6))\n",
        "    ax2.imshow(query_image)\n",
        "    ax2.set_title(\"Cropped query_image\")\n",
        "    ax2.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "    # disconnette click e chiude figura principale\n",
        "    if cid_click is not None:\n",
        "        fig.canvas.mpl_disconnect(cid_click)\n",
        "    disconnect_motion()\n",
        "    plt.close(fig)\n",
        "\n",
        "def on_reset(event):\n",
        "    \"\"\"Cancella la box corrente e ricomincia.\"\"\"\n",
        "    global start_pt, rect_patch, current_box\n",
        "    start_pt = None\n",
        "    current_box = None\n",
        "    disconnect_motion()\n",
        "    if rect_patch is not None:\n",
        "        rect_patch.set_visible(False)\n",
        "    fig.canvas.draw_idle()\n",
        "    print(\"Reset eseguito: ridisegna la box con due click.\")\n",
        "\n",
        "# --- Figura e layout bottoni (stile identico al tuo) ---\n",
        "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
        "plt.subplots_adjust(bottom=0.3)\n",
        "\n",
        "ax_img = ax\n",
        "ax_img.imshow(img)\n",
        "ax_img.set_title(\"Click 1: inizio box | muovi il mouse | Click 2: chiudi box. Poi premi OK.\")\n",
        "ax_img.axis(\"off\")\n",
        "\n",
        "# pulsanti (stessa posizione/estetica del tuo snippet)\n",
        "ax_ok = plt.axes([0.70, 0.05, 0.10, 0.075])\n",
        "ax_reset = plt.axes([0.50, 0.05, 0.15, 0.075])\n",
        "\n",
        "btn_ok = Button(ax_ok, 'OK')\n",
        "btn_reset = Button(ax_reset, 'Reset')\n",
        "\n",
        "# Eventi\n",
        "cid_click = fig.canvas.mpl_connect('button_press_event', onclick)\n",
        "btn_ok.on_clicked(on_ok)\n",
        "btn_reset.on_clicked(on_reset)\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "current_box\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TEST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfZz0h-oRTFa"
      },
      "outputs": [],
      "source": [
        "# https://s.yimg.com/ny/api/res/1.2/beOCT4bnh9ULHdfnLuImrA--/YXBwaWQ9aGlnaGxhbmRlcjt3PTY0MDtoPTQyNztjZj13ZWJw/https://media.zenfs.com/en/warriors_wire_usa_today_sports_articles_759/c34198dce54f9a3b5f3cac1b6e5ef91a\n",
        "test_image = download_image_from_url()\n",
        "if test_image and os.path.exists(test_image):\n",
        "    print(f\"Using image file at: {test_image}\")\n",
        "else:\n",
        "    print(\"No valid image file is available.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CODE TO IMPORT FOUNDATION MODEL "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "import torchvision.transforms as T\n",
        "\n",
        "# -----------------------------\n",
        "# I/O & common utilities\n",
        "# -----------------------------\n",
        "\n",
        "def load_rgb_image(img_path: str) -> Image.Image:\n",
        "    \"\"\"Load an image from disk as RGB with clear errors.\"\"\"\n",
        "    if not os.path.isfile(img_path):\n",
        "        raise FileNotFoundError(f\"Image file not found at '{img_path}'\")\n",
        "    try:\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        return img\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Error opening image: {e}\")\n",
        "\n",
        "def to_numpy(feature_tensor: torch.Tensor) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Convert a feature tensor (1, C, H, W) or (C, H, W) or (H, W) to numpy.\n",
        "    Returns (C, H, W) or (H, W) numpy array.\n",
        "    \"\"\"\n",
        "    if not torch.is_tensor(feature_tensor):\n",
        "        raise TypeError(\"Expected a torch.Tensor\")\n",
        "    x = feature_tensor.detach().cpu()\n",
        "    if x.ndim == 4:\n",
        "        # assume (B, C, H, W)\n",
        "        if x.size(0) != 1:\n",
        "            raise ValueError(f\"Expected batch size 1, got {x.size(0)}\")\n",
        "        x = x.squeeze(0)\n",
        "    return x.numpy()\n",
        "\n",
        "def infer_square_hw_from_seq_len(seq_len: int) -> int:\n",
        "    \"\"\"\n",
        "    Infer H=W from a sequence length that excludes the CLS token.\n",
        "    Raises if not a perfect square.\n",
        "    \"\"\"\n",
        "    side = int(math.sqrt(seq_len))\n",
        "    if side * side != seq_len:\n",
        "        raise ValueError(\n",
        "            f\"Cannot infer square spatial size from sequence length {seq_len}.\"\n",
        "        )\n",
        "    return side\n",
        "\n",
        "def normalize_to_uint8(x: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Min-max normalize any array to [0, 255] uint8 for visualization.\"\"\"\n",
        "    x = x.astype(np.float32)\n",
        "    x_min, x_max = x.min(), x.max()\n",
        "    if x_max == x_min:\n",
        "        return np.zeros_like(x, dtype=np.uint8)\n",
        "    x = (x - x_min) / (x_max - x_min)\n",
        "    x = (x * 255.0).clip(0, 255).astype(np.uint8)\n",
        "    return x\n",
        "\n",
        "# -----------------------------\n",
        "# Transforms per model family\n",
        "# -----------------------------\n",
        "\n",
        "def get_clip_transform():\n",
        "    # CLIP pixel normalization constants\n",
        "    return T.Compose([\n",
        "        T.Resize((224, 224), interpolation=T.InterpolationMode.BICUBIC),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "                    std=[0.26862954, 0.26130258, 0.27577711]),\n",
        "    ])\n",
        "\n",
        "def get_resnet_transform():\n",
        "    return T.Compose([\n",
        "        T.Resize(256),\n",
        "        T.CenterCrop(224),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                    std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "\n",
        "def get_vit_224_transform():\n",
        "    # Generic ViT-friendly transform to 224x224\n",
        "    return T.Compose([\n",
        "        T.Resize((224, 224), interpolation=T.InterpolationMode.BICUBIC),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean=[0.5, 0.5, 0.5],\n",
        "                    std=[0.5, 0.5, 0.5]),\n",
        "    ])\n",
        "\n",
        "def get_sam_preprocessor():\n",
        "    \"\"\"\n",
        "    Returns a callable that resizes + normalizes for SAM using ResizeLongestSide.\n",
        "    We only prepare this if 'segment_anything' is installed.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from segment_anything.utils.transforms import ResizeLongestSide\n",
        "    except Exception as e:\n",
        "        raise ImportError(\n",
        "            \"segment_anything not installed. Install: pip install git+https://github.com/facebookresearch/segment-anything.git\"\n",
        "        )\n",
        "    return ResizeLongestSide(1024)  # standard SAM image size\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Loaders per model family\n",
        "# -----------------------------\n",
        "\n",
        "def load_clip(model_variant: str = \"openai/clip-vit-base-patch32\"):\n",
        "    \"\"\"\n",
        "    Returns (feature_extractor_module, transform_fn, postproc_fn)\n",
        "    where postproc_fn maps model output to (C,H,W) numpy.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from transformers import CLIPModel, CLIPProcessor\n",
        "    except ImportError:\n",
        "        raise ImportError(\"Please install transformers: pip install transformers\")\n",
        "\n",
        "    processor = CLIPProcessor.from_pretrained(model_variant)\n",
        "    model = CLIPModel.from_pretrained(model_variant)\n",
        "    vision = model.vision_model.eval()\n",
        "\n",
        "    transform = get_clip_transform()\n",
        "\n",
        "    def postproc(outputs):\n",
        "        # outputs: BaseModelOutputWithPooling\n",
        "        last = outputs.last_hidden_state  # (B, 1+HW, C)\n",
        "        if last.ndim != 3:\n",
        "            raise ValueError(f\"Unexpected CLIP hidden shape: {last.shape}\")\n",
        "        b, seq, c = last.shape\n",
        "        if b != 1:\n",
        "            raise ValueError(f\"Expected B=1, got {b}\")\n",
        "        spatial_tokens = last[:, 1:, :]  # drop CLS -> (1, HW, C)\n",
        "        hw = spatial_tokens.shape[1]\n",
        "        side = infer_square_hw_from_seq_len(hw)\n",
        "        feat = spatial_tokens[0].transpose(0, 1).reshape(c, side, side)  # (C,H,W)\n",
        "        return to_numpy(feat)\n",
        "\n",
        "    def forward(t: torch.Tensor):\n",
        "        # Vision-only forward\n",
        "        return vision(t)\n",
        "\n",
        "    return forward, transform, postproc\n",
        "\n",
        "def load_resnet50():\n",
        "    \"\"\"Returns a (feature_extractor_module, transform_fn, postproc_fn).\"\"\"\n",
        "    try:\n",
        "        import torchvision.models as models\n",
        "    except ImportError:\n",
        "        raise ImportError(\"Please install torchvision: pip install torchvision\")\n",
        "\n",
        "    backbone = models.resnet50(weights=models.ResNet50_Weights.DEFAULT).eval()\n",
        "    # all children except avgpool+fc -> retain (B, C, H, W)\n",
        "    feat_extractor = torch.nn.Sequential(*(list(backbone.children())[:-2])).eval()\n",
        "    transform = get_resnet_transform()\n",
        "\n",
        "    def postproc(x: torch.Tensor):\n",
        "        # x: (1, C, H, W)\n",
        "        return to_numpy(x)\n",
        "\n",
        "    def forward(t: torch.Tensor):\n",
        "        return feat_extractor(t)\n",
        "\n",
        "    return forward, transform, postproc\n",
        "\n",
        "def load_dinov2(model_variant: str = \"dinov2_vits14\"):\n",
        "    \"\"\"\n",
        "    Load DINOv2 from torch.hub. Valid variants include:\n",
        "    - dinov2_vits14, dinov2_vitb14, dinov2_vitl14, dinov2_vitg14\n",
        "    Returns (forward_fn, transform_fn, postproc_fn).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        model = torch.hub.load(\"facebookresearch/dinov2\", model_variant)\n",
        "    except Exception as e:\n",
        "        raise ImportError(\n",
        "            \"Failed to load DINOv2 via torch.hub. Make sure you have internet access once, \"\n",
        "            \"or have the weights cached. Error: \" + str(e)\n",
        "        )\n",
        "    model.eval()\n",
        "    transform = get_vit_224_transform()\n",
        "\n",
        "    def postproc(outputs: torch.Tensor | dict):\n",
        "        \"\"\"\n",
        "        Some DINOv2 hub models expose forward_features returning a dict;\n",
        "        but the default __call__ returns the class token embedding.\n",
        "        We want spatial tokens. We'll try forward_features if present.\n",
        "        \"\"\"\n",
        "        # Try to get the token features from model.forward_features\n",
        "        tokens = None\n",
        "        with torch.no_grad():\n",
        "            if hasattr(model, \"forward_features\"):\n",
        "                feats = model.forward_features\n",
        "                try:\n",
        "                    out = feats(_last_input_tensor)  # see forward() capturing below\n",
        "                except NameError:\n",
        "                    # fallback: outputs might already be what we need\n",
        "                    out = outputs\n",
        "                if isinstance(out, dict):\n",
        "                    # common keys: 'x_norm_patchtokens' or 'token' etc.\n",
        "                    for k in [\"x_norm_patchtokens\", \"x_norm_patchtokens\"]:\n",
        "                        if k in out:\n",
        "                            tokens = out[k]  # (B, HW, C)\n",
        "                            break\n",
        "                    if tokens is None:\n",
        "                        # Try 'x' or 'tokens'\n",
        "                        for k in [\"x\", \"tokens\"]:\n",
        "                            if k in out:\n",
        "                                tokens = out[k]\n",
        "                                break\n",
        "            # If still None, try to treat outputs as tokens\n",
        "            if tokens is None:\n",
        "                tokens = outputs  # hope it's (B, HW, C) or (B, N, C)\n",
        "\n",
        "        if tokens.ndim != 3:\n",
        "            raise ValueError(\n",
        "                f\"Unexpected DINOv2 token shape: {tokens.shape}. \"\n",
        "                \"You may need to adapt the post-processing for your exact variant.\"\n",
        "            )\n",
        "        b, hw, c = tokens.shape\n",
        "        if b != 1:\n",
        "            raise ValueError(f\"Expected batch size 1, got {b}\")\n",
        "        side = infer_square_hw_from_seq_len(hw)\n",
        "        feat = tokens[0].transpose(0, 1).reshape(c, side, side)\n",
        "        return to_numpy(feat)\n",
        "\n",
        "    # We capture the input tensor inside forward so postproc can use forward_features safely.\n",
        "    def forward(t: torch.Tensor):\n",
        "        global _last_input_tensor\n",
        "        _last_input_tensor = t\n",
        "        # Try to get tokens directly\n",
        "        with torch.no_grad():\n",
        "            if hasattr(model, \"forward_features\"):\n",
        "                out = model.forward_features(t)\n",
        "                if isinstance(out, dict):\n",
        "                    # prefer patch tokens if provided\n",
        "                    for k in [\"x_norm_patchtokens\", \"tokens\", \"x\"]:\n",
        "                        if k in out:\n",
        "                            return out[k]  # (B, HW, C)\n",
        "                return out  # might already be (B, HW, C)\n",
        "            # fallback to model(t) (often returns cls embedding, not ideal)\n",
        "            return model(t)\n",
        "\n",
        "    return forward, transform, postproc\n",
        "\n",
        "def load_dinov3(model_variant: str = \"vit_base_patch16_224.dino\", hf_vit_repo: str | None = None):\n",
        "    \"\"\"\n",
        "    Prefer loading a ViT from timm and force extraction of patch tokens.\n",
        "    Falls back to Hugging Face ViT if timm isn't available or fails.\n",
        "    Returns (forward_fn, transform_fn, postproc_fn).\n",
        "    \"\"\"\n",
        "    import inspect\n",
        "\n",
        "    def _call_with_supported_kwargs(fn, *args, **kwargs):\n",
        "        \"\"\"Call fn with only kwargs it supports (by name).\"\"\"\n",
        "        sig = inspect.signature(fn)\n",
        "        supported = {k: v for k, v in kwargs.items() if k in sig.parameters}\n",
        "        return fn(*args, **supported)\n",
        "\n",
        "    try:\n",
        "        import timm\n",
        "        model = timm.create_model(model_variant, pretrained=True)\n",
        "        model.eval()\n",
        "        transform = get_vit_224_transform()\n",
        "\n",
        "        # --- Strategy: try to get tokens directly; else hook the last block ---\n",
        "        last_block_tokens = {}\n",
        "\n",
        "        def _last_block_hook(module, inp, out):\n",
        "            # out is (B, N, C) for ViT blocks\n",
        "            last_block_tokens[\"x\"] = out\n",
        "\n",
        "        # Find a reasonable \"last block\" to hook\n",
        "        if hasattr(model, \"blocks\") and len(getattr(model, \"blocks\")) > 0:\n",
        "            hooked_module = model.blocks[-1]\n",
        "            h = hooked_module.register_forward_hook(_last_block_hook)\n",
        "\n",
        "        def _extract_tokens_with_hooks(x: torch.Tensor):\n",
        "            \"\"\"Run a regular forward to trigger the hook and retrieve tokens.\"\"\"\n",
        "            last_block_tokens.clear()\n",
        "            if hasattr(model, \"forward_features\"):\n",
        "                _ = model.forward_features(x)  # triggers hook\n",
        "            else:\n",
        "                _ = model(x)\n",
        "            if \"x\" not in last_block_tokens:\n",
        "                raise ValueError(\"Failed to capture tokens from the last transformer block.\")\n",
        "            return last_block_tokens[\"x\"]  # (B, N, C)\n",
        "\n",
        "        def forward(t: torch.Tensor):\n",
        "            with torch.no_grad():\n",
        "                # 1) Try forward_features with return_dict=True (if supported)\n",
        "                if hasattr(model, \"forward_features\"):\n",
        "                    try:\n",
        "                        out = _call_with_supported_kwargs(model.forward_features, t, return_dict=True)\n",
        "                        if isinstance(out, dict):\n",
        "                            tok = out.get(\"x\", None)\n",
        "                            if tok is None:\n",
        "                                for k in [\"tokens\", \"features\"]:\n",
        "                                    if k in out:\n",
        "                                        tok = out[k]; break\n",
        "                            if tok is not None and tok.ndim == 3:\n",
        "                                if tok.shape[1] >= 2:\n",
        "                                    tok = tok[:, 1:, :]\n",
        "                                return tok  # (B, HW, C)\n",
        "                    except Exception:\n",
        "                        pass\n",
        "\n",
        "                    # 2) Try return_all_tokens=True if supported\n",
        "                    try:\n",
        "                        out = _call_with_supported_kwargs(model.forward_features, t, return_all_tokens=True)\n",
        "                        if isinstance(out, torch.Tensor) and out.ndim == 3:\n",
        "                            if out.shape[1] >= 2:\n",
        "                                out = out[:, 1:, :]\n",
        "                            return out\n",
        "                        if isinstance(out, dict):\n",
        "                            tok = out.get(\"x\", None)\n",
        "                            if tok is not None and tok.ndim == 3:\n",
        "                                if tok.shape[1] >= 2:\n",
        "                                    tok = tok[:, 1:, :]\n",
        "                                return tok\n",
        "                    except Exception:\n",
        "                        pass\n",
        "\n",
        "                # 3) Fallback: use hook to grab tokens from last block output\n",
        "                tok = _extract_tokens_with_hooks(t)\n",
        "                if tok.ndim == 3 and tok.shape[1] >= 2:\n",
        "                    tok = tok[:, 1:, :]\n",
        "                return tok\n",
        "\n",
        "        def postproc(tokens: torch.Tensor):\n",
        "            if tokens.ndim != 3:\n",
        "                raise ValueError(f\"Unexpected token tensor shape from timm model: {tokens.shape}\")\n",
        "            b, hw, c = tokens.shape\n",
        "            if b != 1:\n",
        "                raise ValueError(f\"Expected batch size 1, got {b}\")\n",
        "            side = infer_square_hw_from_seq_len(hw)\n",
        "            feat = tokens[0].transpose(0, 1).reshape(c, side, side)\n",
        "            return to_numpy(feat)\n",
        "\n",
        "        return forward, transform, postproc\n",
        "\n",
        "    except Exception as timm_err:\n",
        "        # ---- HF fallback (always returns tokens) ----\n",
        "        if hf_vit_repo is None:\n",
        "            raise ImportError(\n",
        "                \"timm not available or model not found, and no Hugging Face ViT repo provided. \"\n",
        "                \"Install timm (pip install timm) or pass hf_vit_repo.\\n\"\n",
        "                f\"Original error: {timm_err}\"\n",
        "            )\n",
        "\n",
        "        try:\n",
        "            from transformers import ViTModel, AutoImageProcessor\n",
        "        except ImportError:\n",
        "            raise ImportError(\"Please install transformers: pip install transformers\")\n",
        "\n",
        "        processor = AutoImageProcessor.from_pretrained(hf_vit_repo)\n",
        "        vit = ViTModel.from_pretrained(hf_vit_repo).eval()\n",
        "        transform = get_vit_224_transform()\n",
        "\n",
        "        def forward(t: torch.Tensor):\n",
        "            with torch.no_grad():\n",
        "                outputs = vit(pixel_values=t)\n",
        "            return outputs.last_hidden_state  # (B, 1+HW, C)\n",
        "\n",
        "        def postproc(last_hidden: torch.Tensor):\n",
        "            if last_hidden.ndim != 3:\n",
        "                raise ValueError(f\"Unexpected ViT hidden shape: {last_hidden.shape}\")\n",
        "            b, n, c = last_hidden.shape\n",
        "            if b != 1:\n",
        "                raise ValueError(f\"Expected batch size 1, got {b}\")\n",
        "            tokens = last_hidden[:, 1:, :]  # drop CLS\n",
        "            hw = tokens.shape[1]\n",
        "            side = infer_square_hw_from_seq_len(hw)\n",
        "            feat = tokens[0].transpose(0, 1).reshape(c, side, side)\n",
        "            return to_numpy(feat)\n",
        "\n",
        "        return forward, transform, postproc\n",
        "\n",
        "def load_sam(checkpoint_path: str, model_type: str = \"vit_h\"):\n",
        "    \"\"\"\n",
        "    Load SAM and return (forward_fn, transform_fn, postproc_fn).\n",
        "    This version:\n",
        "      - uses sam.preprocess() to normalize+pad to 1024x1024,\n",
        "      - crops the valid (unpadded) embedding region,\n",
        "      - resizes embeddings back to the original image size.\n",
        "    Final output: (C, H0, W0) aligned to the input image.\n",
        "    \"\"\"\n",
        "    import os\n",
        "    import numpy as np\n",
        "    import torch\n",
        "    import torch.nn.functional as F\n",
        "\n",
        "    try:\n",
        "        from segment_anything import sam_model_registry\n",
        "        from segment_anything.utils.transforms import ResizeLongestSide\n",
        "    except ImportError:\n",
        "        raise ImportError(\n",
        "            \"Please install Segment Anything:\\n\"\n",
        "            \"pip install git+https://github.com/facebookresearch/segment-anything.git\"\n",
        "        )\n",
        "\n",
        "    if not os.path.isfile(checkpoint_path):\n",
        "        raise FileNotFoundError(\n",
        "            f\"SAM checkpoint not found at '{checkpoint_path}'. \"\n",
        "        )\n",
        "\n",
        "    if model_type not in {\"vit_h\", \"vit_l\", \"vit_b\"}:\n",
        "        raise ValueError(\"SAM model_type must be 'vit_h', 'vit_l' or 'vit_b'.\")\n",
        "\n",
        "    sam = sam_model_registry[model_type](checkpoint=checkpoint_path).eval()\n",
        "    resizer = ResizeLongestSide(1024)\n",
        "\n",
        "    # --- closure state to carry geometry across transform/forward/postproc ---\n",
        "    orig_hw = {\"H0\": None, \"W0\": None}   # original image size\n",
        "    rez_hw  = {\"Hr\": None, \"Wr\": None}   # resized (before padding) size\n",
        "\n",
        "    def transform_for_sam(pil_img: \"Image.Image\") -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Returns (1, 3, H', W') float32 in range [0..255].\n",
        "        Stores original (H0,W0) and resized (Hr,Wr) in the closure.\n",
        "        \"\"\"\n",
        "        image_np = np.array(pil_img)  # (H0, W0, 3) uint8\n",
        "        H0, W0 = image_np.shape[:2]\n",
        "        orig_hw[\"H0\"], orig_hw[\"W0\"] = H0, W0\n",
        "\n",
        "        # resize (keep aspect) so that max(Hr,Wr)=1024; still no padding here\n",
        "        resized_np = resizer.apply_image(image_np)  # (Hr, Wr, 3)\n",
        "        Hr, Wr = resized_np.shape[:2]\n",
        "        rez_hw[\"Hr\"], rez_hw[\"Wr\"] = Hr, Wr\n",
        "\n",
        "        t = torch.as_tensor(resized_np, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0)\n",
        "        return t  # (1, 3, Hr, Wr) in [0..255]\n",
        "\n",
        "    def forward(t: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Use sam.preprocess() to normalize+pad to (1,3,1024,1024), then image_encoder().\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            device = next(sam.parameters()).device\n",
        "            x = t.to(device)          # (1, 3, Hr, Wr), 0..255\n",
        "            x = sam.preprocess(x)     # (1, 3, 1024, 1024)\n",
        "            emb = sam.image_encoder(x)  # (1, C, 64, 64)\n",
        "            return emb\n",
        "\n",
        "    def postproc(emb: torch.Tensor):\n",
        "        \"\"\"\n",
        "        1) cut away bottom/right padding,\n",
        "        2) upsample to (Hr,Wr),\n",
        "        3) resize to (H0,W0).\n",
        "        Returns -> (C, H0, W0) numpy.\n",
        "        \"\"\"\n",
        "        if orig_hw[\"H0\"] is None or rez_hw[\"Hr\"] is None:\n",
        "            raise RuntimeError(\"Geometry metadata missing. Did you call transform_for_sam first?\")\n",
        "\n",
        "        H0, W0 = orig_hw[\"H0\"], orig_hw[\"W0\"]\n",
        "        Hr, Wr = rez_hw[\"Hr\"], rez_hw[\"Wr\"]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # emb: (1, C, 64, 64)\n",
        "            B, C, Hf, Wf = emb.shape\n",
        "            if B != 1:\n",
        "                raise ValueError(f\"Expected batch size 1, got {B}\")\n",
        "\n",
        "            # valid token counts BEFORE padding (padding is bottom/right)\n",
        "            # each token covers 16x16 input pixels (1024/64)\n",
        "            ht = int(np.ceil(Hr / 16))\n",
        "            wt = int(np.ceil(Wr / 16))\n",
        "            ht = min(ht, Hf)\n",
        "            wt = min(wt, Wf)\n",
        "\n",
        "            emb_valid = emb[:, :, :ht, :wt]  # (1, C, ht, wt)\n",
        "\n",
        "            # upsample from tokens to resized image resolution (Hr, Wr)\n",
        "            emb_resized = F.interpolate(emb_valid, size=(Hr, Wr), mode=\"bilinear\", align_corners=False)  # (1, C, Hr, Wr)\n",
        "\n",
        "            # finally, map back to original resolution (H0, W0)\n",
        "            emb_original = F.interpolate(emb_resized, size=(H0, W0), mode=\"bilinear\", align_corners=False)  # (1, C, H0, W0)\n",
        "\n",
        "        return emb_original.squeeze(0).cpu().numpy()  # (C, H0, W0)\n",
        "\n",
        "    return (forward, transform_for_sam, postproc)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def visualize_overlay_mean(img: Image.Image, features_chw: np.ndarray, alpha: float = 0.5, title: str = \"\"):\n",
        "    if features_chw.ndim != 3:\n",
        "        raise ValueError(f\"Expected (C,H,W) features, got {features_chw.shape}\")\n",
        "    feat_mean = np.mean(features_chw, axis=0)  # (H, W)\n",
        "    feat_u8 = normalize_to_uint8(feat_mean)\n",
        "    heatmap = Image.fromarray(feat_u8).resize(img.size, resample=Image.BICUBIC)\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(img)\n",
        "    plt.title(\"Original Image\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(img)\n",
        "    plt.imshow(heatmap, cmap=\"viridis\", alpha=alpha)\n",
        "    plt.title(title or \"Features Overlay (Mean)\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "def visualize_overlay_pca(img: Image.Image, features_chw: np.ndarray, alpha: float = 0.98, title: str = \"\"):\n",
        "    if features_chw.ndim != 3 or features_chw.shape[0] < 3:\n",
        "        raise ValueError(f\"PCA visualization requires features with >=3 channels, got {features_chw.shape}\")\n",
        "\n",
        "    C, H, W = features_chw.shape\n",
        "    X = features_chw.transpose(1, 2, 0).reshape(-1, C)  # (H*W, C)\n",
        "    pca = PCA(n_components=3)\n",
        "    X3 = pca.fit_transform(X)  # (H*W, 3)\n",
        "    X3 = X3.reshape(H, W, 3)\n",
        "    X3_u8 = normalize_to_uint8(X3)\n",
        "    rgb = Image.fromarray(X3_u8).resize(img.size, resample=Image.BICUBIC)\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(img)\n",
        "    plt.title(\"Original Image\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(img)\n",
        "    plt.imshow(rgb, alpha=alpha)\n",
        "    plt.title(title or \"Features Overlay (PCA)\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "def visualize_features(\n",
        "    img: Image.Image,\n",
        "    features_chw: np.ndarray,\n",
        "    mode: str = \"mean\",\n",
        "    title_prefix: str = \"\"\n",
        "):\n",
        "    # --- fallback to the generic mean/pca overlays ---\n",
        "    if features_chw.ndim == 3:\n",
        "        if mode == \"mean\":\n",
        "            visualize_overlay_mean(img, features_chw, alpha=0.5, title=f\"{title_prefix} Features (Mean)\")\n",
        "        elif mode == \"pca\":\n",
        "            visualize_overlay_pca(img, features_chw, alpha=0.98, title=f\"{title_prefix} Features (PCA)\")\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported visualization mode: {mode}\")\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Cannot visualize features of shape {features_chw.shape}\")\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Orchestrator\n",
        "# -----------------------------\n",
        "\n",
        "def get_backbone_and_transform(\n",
        "    model: str,\n",
        "    model_variant: str | None = None,\n",
        "    sam_checkpoint: str | None = None,\n",
        "    dinov3_hf_vit_repo: str | None = None\n",
        "):\n",
        "    \"\"\"\n",
        "    Return (forward_fn, transform_fn, postproc_fn, model_label) for the requested model.\n",
        "    model: 'clip' | 'resnet' | 'dinov2' | 'dinov3' | 'sam' | 'handcrafted'\n",
        "    model_variant: optional string to pick a specific sub-variant.\n",
        "    sam_checkpoint: required for 'sam'.\n",
        "    dinov3_hf_vit_repo: optional HF repo id for ViT fallback if timm variant isn't available.\n",
        "    \"\"\"\n",
        "    m = model.strip().lower()\n",
        "    if m == \"clip\":\n",
        "        fwd, tfm, post = load_clip(model_variant or \"openai/clip-vit-base-patch32\")\n",
        "        return fwd, tfm, post, f\"CLIP ({model_variant or 'ViT-B/32'})\"\n",
        "    elif m == \"resnet\":\n",
        "        fwd, tfm, post = load_resnet50()\n",
        "        return fwd, tfm, post, \"ResNet50 (pretrained)\"\n",
        "    elif m == \"dinov2\":\n",
        "        fwd, tfm, post = load_dinov2(model_variant or \"dinov2_vits14\")\n",
        "        return fwd, tfm, post, f\"DINOv2 ({model_variant or 'dinov2_vits14'})\"\n",
        "    elif m == \"dinov3\":\n",
        "        fwd, tfm, post = load_dinov3(model_variant or \"vit_base_patch16_224.dino\",\n",
        "                                     hf_vit_repo=dinov3_hf_vit_repo)\n",
        "        label = f\"DINOv3/ViT ({model_variant or dinov3_hf_vit_repo or 'timm default'})\"\n",
        "        return fwd, tfm, post, label\n",
        "    elif m == \"sam\":\n",
        "        if not sam_checkpoint:\n",
        "            raise ValueError(\"For 'sam', you must pass sam_checkpoint='path/to/sam_checkpoint.pth'.\")\n",
        "        fwd, tfm, post = load_sam(sam_checkpoint, model_variant or \"vit_h\")\n",
        "        return fwd, tfm, post, f\"SAM ({model_variant or 'vit_h'})\"\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported model. Choose from: 'clip', 'resnet', 'dinov2', 'dinov3', 'sam', 'handcrafted'.\")\n",
        "\n",
        "def extract_spatial_features_from_image(\n",
        "    img: Image.Image,\n",
        "    forward_fn,\n",
        "    transform_fn,\n",
        "    postproc_fn\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Apply transform, run forward, and convert to (C, H, W) numpy spatial map.\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        img_t = transform_fn(img)  # (1, C, H, W) or SAM's input\n",
        "        if img_t.ndim == 3:\n",
        "            img_t = img_t.unsqueeze(0)\n",
        "        outputs = forward_fn(img_t)\n",
        "    features = postproc_fn(outputs)\n",
        "    if features.ndim == 2:\n",
        "        # (H, W) -> add channel dim\n",
        "        features = features[None, ...]\n",
        "    if features.ndim != 3:\n",
        "        raise ValueError(f\"Expected (C,H,W) after postproc, got {features.shape}\")\n",
        "    return features\n",
        "\n",
        "# -----------------------------\n",
        "# Public API (drop-in for your original function)\n",
        "# -----------------------------\n",
        "\n",
        "def spatial_features_extractor(\n",
        "    img_path: str,\n",
        "    model: str = \"clip\",\n",
        "    visualization_mode: str = \"mean\",\n",
        "    *,\n",
        "    model_variant: str | None = None,\n",
        "    sam_checkpoint: str | None = None,\n",
        "    dinov3_hf_vit_repo: str | None = None\n",
        "):\n",
        "    \"\"\"\n",
        "    Extract and visualize spatial features from an image using different models.\n",
        "\n",
        "    Args:\n",
        "        img_path: Path to the image file.\n",
        "        model: One of 'clip', 'resnet', 'dinov2', 'dinov3', 'sam', 'handcrafted'.\n",
        "        visualization_mode: 'mean' or 'pca' or any handcrafted-specific mode routed by visualize_features.\n",
        "        model_variant:\n",
        "            - clip: HF model id (default 'openai/clip-vit-base-patch32')\n",
        "            - resnet: ignored\n",
        "            - dinov2: torch.hub variants (e.g., 'dinov2_vits14', 'dinov2_vitb14', ...)\n",
        "            - dinov3: timm name (e.g., 'vit_base_patch16_224.dino'), or use dinov3_hf_vit_repo\n",
        "            - sam: 'vit_h' | 'vit_l' | 'vit_b'\n",
        "            - handcrafted: 'hog' | 'lbp' | 'gabor' | 'sobel' | 'sift' | 'orb'\n",
        "        sam_checkpoint: required for 'sam' (path to .pth checkpoint).\n",
        "        dinov3_hf_vit_repo:\n",
        "            If timm is unavailable or your timm variant isn't found, provide a Hugging Face ViT repo id\n",
        "            for fallback (any ViT that returns token sequences), e.g., 'google/vit-base-patch16-224'.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        img = load_rgb_image(img_path)\n",
        "    except Exception as e:\n",
        "        print(str(e))\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        forward_fn, transform_fn, postproc_fn, label = get_backbone_and_transform(\n",
        "            model=model,\n",
        "            model_variant=model_variant,\n",
        "            sam_checkpoint=sam_checkpoint,\n",
        "            dinov3_hf_vit_repo=dinov3_hf_vit_repo\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model '{model}': {e}\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        features = extract_spatial_features_from_image(img, forward_fn, transform_fn, postproc_fn)\n",
        "        print(f\"Spatial features shape: {features.shape}\")  # (C, H, W)\n",
        "    except Exception as e:\n",
        "        print(f\"Error during feature extraction: {e}\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        visualize_features(img, features, mode=visualization_mode, title_prefix=label)\n",
        "    except Exception as e:\n",
        "        print(f\"Error during visualization: {e}\")\n",
        "        return\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# spatial_features_extractor(test_image, model=\"clip\", visualization_mode=\"pca\")\n",
        "# spatial_features_extractor(test_image, model=\"resnet\", visualization_mode=\"pca\")\n",
        "# spatial_features_extractor(test_image, model=\"dinov2\", model_variant=\"dinov2_vitb14\", visualization_mode=\"pca\")\n",
        "# spatial_features_extractor(test_image, model=\"dinov3\", model_variant=\"vit_base_patch16_224.dino\", visualization_mode=\"pca\")\n",
        "# spatial_features_extractor(test_image, model=\"dinov3\", dinov3_hf_vit_repo=\"google/vit-base-patch16-224\", visualization_mode=\"pca\")\n",
        "# spatial_features_extractor(test_image, model=\"sam\", model_variant=\"vit_h\", sam_checkpoint=\"../content/sam_vit_h_4b8939.pth\", visualization_mode=\"pca\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# FUNCTIONS for TEMPLATE MATCHING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# Helpers minimi\n",
        "# -----------------------------\n",
        "def ensure_pil(img):\n",
        "    \"\"\"Accetta PIL o np.ndarray (H,W,3/4) e restituisce PIL.Image RGB.\"\"\"\n",
        "    if isinstance(img, Image.Image):\n",
        "        return img.convert(\"RGB\")\n",
        "    if isinstance(img, np.ndarray):\n",
        "        x = img\n",
        "        if x.ndim != 3 or x.shape[2] not in (3, 4):\n",
        "            raise TypeError(f\"Unsupported ndarray shape for image: {x.shape}\")\n",
        "        if x.dtype != np.uint8:\n",
        "            x = np.clip(x, 0, 255).astype(np.uint8)\n",
        "        if x.shape[2] == 4:\n",
        "            x = x[:, :, :3]\n",
        "        return Image.fromarray(x, mode=\"RGB\")\n",
        "    raise TypeError(f\"Unsupported image type: {type(img)}\")\n",
        "\n",
        "def infer_square_hw_from_seq_len(seq_len: int) -> int:\n",
        "    side = int(round(seq_len ** 0.5))\n",
        "    if side * side != seq_len:\n",
        "        raise ValueError(f\"Cannot infer (H=W) from seq_len={seq_len}\")\n",
        "    return side\n",
        "\n",
        "def get_vit_224_transform():\n",
        "    return T.Compose([\n",
        "        T.Resize((224, 224), interpolation=T.InterpolationMode.BICUBIC),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "    ])\n",
        "\n",
        "# -----------------------------\n",
        "# Loader DINOv3 (timm) minimale\n",
        "# -----------------------------\n",
        "def load_dinov3_minimal(model_variant: str = \"vit_base_patch16_224.dino\"):\n",
        "    \"\"\"\n",
        "    Crea un ViT DINOv3 da timm e restituisce:\n",
        "      forward_fn(t)->tokens (B, HW, C)\n",
        "      transform_fn(PIL)->(1,3,224,224)\n",
        "      postproc_fn(tokens)->(C,H,W) numpy\n",
        "    \"\"\"\n",
        "    import timm\n",
        "\n",
        "    model = timm.create_model(model_variant, pretrained=True)\n",
        "    model.eval()\n",
        "\n",
        "    transform = get_vit_224_transform()\n",
        "\n",
        "    # Hook sull'ultimo blocco per catturare i token\n",
        "    last_tokens = {}\n",
        "    if hasattr(model, \"blocks\") and len(model.blocks) > 0:\n",
        "        h = model.blocks[-1].register_forward_hook(lambda m, i, o: last_tokens.__setitem__(\"x\", o))\n",
        "\n",
        "    def forward(t: torch.Tensor):\n",
        "        with torch.no_grad():\n",
        "            last_tokens.clear()\n",
        "            if hasattr(model, \"forward_features\"):\n",
        "                _ = model.forward_features(t)\n",
        "            else:\n",
        "                _ = model(t)\n",
        "            x = last_tokens.get(\"x\", None)            # (B, N, C)\n",
        "            if x is None:\n",
        "                raise RuntimeError(\"Failed to capture ViT tokens from last block.\")\n",
        "            if x.ndim != 3:\n",
        "                raise ValueError(f\"Unexpected tokens shape: {x.shape}\")\n",
        "            # drop CLS se presente\n",
        "            if x.shape[1] >= 2:\n",
        "                x = x[:, 1:, :]\n",
        "            return x  # (B, HW, C)\n",
        "\n",
        "    def postproc(tokens: torch.Tensor) -> np.ndarray:\n",
        "        if tokens.ndim != 3:\n",
        "            raise ValueError(f\"Unexpected token tensor shape: {tokens.shape}\")\n",
        "        b, hw, c = tokens.shape\n",
        "        if b != 1:\n",
        "            raise ValueError(f\"Expected B=1, got {b}\")\n",
        "        side = infer_square_hw_from_seq_len(hw)\n",
        "        feat = tokens[0].transpose(0, 1).reshape(c, side, side)  # (C,H,W)\n",
        "        return feat.detach().cpu().numpy()\n",
        "\n",
        "    return forward, transform, postproc\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Estrazione feature (comune)\n",
        "# -----------------------------\n",
        "def extract_spatial_features_from_image(img_pil: Image.Image, forward_fn, transform_fn, postproc_fn) -> np.ndarray:\n",
        "    with torch.no_grad():\n",
        "        t = transform_fn(img_pil)  # (3,H,W) norm\n",
        "        if t.ndim == 3:\n",
        "            t = t.unsqueeze(0)\n",
        "        out = forward_fn(t)        # tokens/feat\n",
        "    feat = postproc_fn(out)        # (C,H,W) numpy\n",
        "    if feat.ndim == 2:\n",
        "        feat = feat[None, ...]\n",
        "    if feat.ndim != 3:\n",
        "        raise ValueError(f\"Expected (C,H,W), got {feat.shape}\")\n",
        "    return feat\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Matching & visual\n",
        "# -----------------------------\n",
        "def resize_features_chw(features_chw: np.ndarray, out_hw: tuple[int, int]) -> np.ndarray:\n",
        "    if features_chw.ndim != 3:\n",
        "        raise ValueError(f\"Expected (C,H,W), got {features_chw.shape}\")\n",
        "    C, H, W = features_chw.shape\n",
        "    Ht, Wt = out_hw\n",
        "    t = torch.from_numpy(features_chw).float().unsqueeze(0)  # (1,C,H,W)\n",
        "    t = F.interpolate(t, size=(int(Ht), int(Wt)), mode=\"bilinear\", align_corners=False)\n",
        "    return t.squeeze(0).cpu().numpy()\n",
        "\n",
        "def cosine_template_match(test_feat_chw: np.ndarray, templ_feat_chw: np.ndarray, stride: int = 1) -> np.ndarray:\n",
        "    if test_feat_chw.ndim != 3 or templ_feat_chw.ndim != 3:\n",
        "        raise ValueError(\"Both feature maps must be (C,H,W).\")\n",
        "    Ct, Ht, Wt = test_feat_chw.shape\n",
        "    Ck, Hk, Wk = templ_feat_chw.shape\n",
        "    if Ct != Ck:\n",
        "        raise ValueError(f\"Channel mismatch: test C={Ct} vs templ C={Ck}\")\n",
        "\n",
        "    x = torch.from_numpy(test_feat_chw).float().unsqueeze(0)  # (1,C,Ht,Wt)\n",
        "    k = torch.from_numpy(templ_feat_chw).float().unsqueeze(0) # (1,C,Hk,Wk)\n",
        "\n",
        "    patches = F.unfold(x, kernel_size=(Hk, Wk), stride=stride)  # (1, C*Hk*Wk, L)\n",
        "    v = k.view(1, -1, 1)                                       # (1, C*Hk*Wk, 1)\n",
        "\n",
        "    dots = (patches * v).sum(dim=1, keepdim=True)              # (1,1,L)\n",
        "    v_norm = torch.norm(v, dim=1, keepdim=True)                # (1,1,1)\n",
        "    p_norm = torch.norm(patches, dim=1, keepdim=True) + 1e-8   # (1,1,L)\n",
        "    cos = dots / (v_norm * p_norm + 1e-8)                      # (1,1,L)\n",
        "\n",
        "    Hout = (Ht - Hk) // stride + 1\n",
        "    Wout = (Wt - Wk) // stride + 1\n",
        "    return cos.view(1, 1, Hout, Wout).squeeze().cpu().numpy()  # (Hout, Wout)\n",
        "\n",
        "def visualize_similarity_heatmap(test_img: Image.Image, sim_map: np.ndarray, alpha: float = 0.65, title: str = \"Cosine Similarity — DINOv3\"):\n",
        "    if sim_map.ndim != 2:\n",
        "        raise ValueError(f\"sim_map must be (H,W), got {sim_map.shape}\")\n",
        "    Himg, Wimg = test_img.size[1], test_img.size[0]\n",
        "    sm = sim_map.astype(np.float32)\n",
        "    sm_min, sm_max = float(sm.min()), float(sm.max())\n",
        "    if sm_max == sm_min:\n",
        "        sm_n = np.zeros_like(sm, dtype=np.uint8)\n",
        "    else:\n",
        "        sm_n = ((sm - sm_min) / (sm_max - sm_min) * 255.0).clip(0, 255).astype(np.uint8)\n",
        "    heat = Image.fromarray(sm_n).resize((Wimg, Himg), resample=Image.BICUBIC)\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(test_img)\n",
        "    plt.title(\"Test Image\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(test_img)\n",
        "    plt.imshow(heat, cmap=\"viridis\", alpha=alpha)\n",
        "    plt.title(title)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "# -----------------------------\n",
        "# Funzione principale richiesta\n",
        "# -----------------------------\n",
        "def cosine_similarity_from_bbox(\n",
        "    query_image,                      # PIL o np.ndarray (RGB)\n",
        "    bbox: tuple[int, int, int, int],  # (x0,y0,x1,y1) in pixel sulla query_image\n",
        "    test_image,                       # PIL o np.ndarray (RGB)\n",
        "    *,\n",
        "    model: str = \"dinov3\",\n",
        "    model_variant: str = \"vit_base_patch16_224.dino\",\n",
        "    template_target_hw: tuple[int, int] | None = (9, 9),\n",
        "    stride: int = 1,\n",
        "    visualize: bool = True,\n",
        "):\n",
        "    \"\"\"\n",
        "    Estrae il crop da query_image via bbox, calcola feature (C,H,W) per crop e test_image\n",
        "    usando DINOv3 (timm), poi esegue dense cosine template matching in feature-space.\n",
        "    \"\"\"\n",
        "    # 1) Preparazione immagini\n",
        "    q_pil = ensure_pil(query_image)\n",
        "    t_pil = ensure_pil(test_image)\n",
        "\n",
        "    x0, y0, x1, y1 = map(int, bbox)\n",
        "    Wq, Hq = q_pil.size\n",
        "    x0 = max(0, min(x0, Wq - 1)); x1 = max(0, min(x1, Wq - 1))\n",
        "    y0 = max(0, min(y0, Hq - 1)); y1 = max(0, min(y1, Hq - 1))\n",
        "    if x1 <= x0 or y1 <= y0:\n",
        "        raise ValueError(f\"BBox non valida: {bbox}\")\n",
        "    crop_pil = q_pil.crop((x0, y0, x1, y1))\n",
        "\n",
        "    # 2) Backbone (supportiamo il caso richiesto: dinov3 via timm)\n",
        "    if model.lower() != \"dinov3\":\n",
        "        raise ValueError(\"Questa versione minimale implementa solo 'dinov3' (timm).\")\n",
        "    forward_fn, transform_fn, postproc_fn = load_dinov3_minimal(model_variant=model_variant)\n",
        "\n",
        "    # 3) Estrazione feature\n",
        "    templ_feat = extract_spatial_features_from_image(crop_pil, forward_fn, transform_fn, postproc_fn)  # (C,hk,wk)\n",
        "    test_feat  = extract_spatial_features_from_image(t_pil,   forward_fn, transform_fn, postproc_fn)  # (C,ht,wt)\n",
        "\n",
        "    # (Opz.) ridimensiona il template in feature-space\n",
        "    if template_target_hw is not None:\n",
        "        Htgt, Wtgt = template_target_hw\n",
        "        templ_feat = resize_features_chw(templ_feat, (max(3, int(Htgt)), max(3, int(Wtgt))))\n",
        "\n",
        "    # 4) Cosine template matching\n",
        "    sim_map = cosine_template_match(test_feat, templ_feat, stride=stride)  # (H',W')\n",
        "\n",
        "    # 5) Peak & visual\n",
        "    y_f, x_f = np.unravel_index(sim_map.argmax(), sim_map.shape)  # coords nel feature-space\n",
        "    if visualize:\n",
        "        visualize_similarity_heatmap(t_pil, sim_map, alpha=0.65,\n",
        "                                     title=f\"Cosine Similarity — DINOv3 ({model_variant})\")\n",
        "        # opzionale: punto del picco\n",
        "        plt.figure(figsize=(5,5))\n",
        "        plt.imshow(t_pil); plt.axis(\"off\")\n",
        "        # upsample grezzo sim_map per mostrare il picco in pixel\n",
        "        sm = sim_map.astype(np.float32)\n",
        "        sm_n = (255.0 * (sm - sm.min()) / (sm.max() - sm.min() + 1e-8)).astype(np.uint8)\n",
        "        up = Image.fromarray(sm_n).resize(t_pil.size, resample=Image.BICUBIC)\n",
        "        up_np = np.array(up); yy, xx = np.unravel_index(up_np.argmax(), up_np.shape)\n",
        "        plt.scatter([xx], [yy], s=80, c='red', marker='+')\n",
        "        plt.title(f\"Peak cos={sim_map.max():.3f}\")\n",
        "        plt.show()\n",
        "\n",
        "    return {\n",
        "        \"sim_map\": sim_map,\n",
        "        \"query_feat_shape\": tuple(templ_feat.shape),\n",
        "        \"test_feat_shape\": tuple(test_feat.shape),\n",
        "        \"peak_feature_xy\": (int(y_f), int(x_f)),\n",
        "        \"bbox\": (x0, y0, x1, y1),\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # old \n",
        "\n",
        "# import numpy as np\n",
        "# import torch\n",
        "# import torch.nn.functional as F\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# def cosine_similarity_from_bbox(\n",
        "#     query_image,                      # PIL.Image o np.ndarray RGB (H,W,3)\n",
        "#     bbox: tuple[int, int, int, int],  # (x0, y0, x1, y1) in pixel dell'immagine query\n",
        "#     test_image,                       # PIL.Image o np.ndarray RGB\n",
        "#     *,\n",
        "#     model: str = \"dinov2\",            # 'clip' | 'resnet' | 'dinov2' | 'dinov3' | 'sam'\n",
        "#     model_variant: str | None = None, # es. 'dinov2_vits14', 'openai/clip-vit-base-patch32', 'vit_base_patch16_224.dino'\n",
        "#     sam_checkpoint: str | None = None,# richiesto se model == 'sam'\n",
        "#     dinov3_hf_vit_repo: str | None = None,  # fallback HF per dinov3 se timm non disponibile\n",
        "#     template_target_hw: tuple[int, int] | None = (9, 9),  # ridimensiona il template in feature-space (stabilizza/velocizza)\n",
        "#     stride: int = 1,                  # stride in feature-space per il matching\n",
        "#     visualize: bool = True            # se True, disegna heatmap sulla test image\n",
        "# ):\n",
        "#     \"\"\"\n",
        "#     Estrae il crop dalla query_image via bbox, calcola feature (C,H,W) per crop e test_image\n",
        "#     con la backbone scelta, poi esegue dense cosine template matching nello spazio feature.\n",
        "\n",
        "#     Ritorna:\n",
        "#         {\n",
        "#             'sim_map': np.ndarray (H', W') in [-1,1],           # mappa di similarità in feature-space\n",
        "#             'label': str,                                       # etichetta backbone\n",
        "#             'query_feat_shape': tuple,                          # (C, hq, wq)\n",
        "#             'test_feat_shape': tuple,                           # (C, ht, wt)\n",
        "#             'peak_feature_xy': (y_f, x_f),                      # posizione picco in sim_map (feature-space)\n",
        "#             'peak_image_xy': (y_img, x_img),                    # posizione picco riportata in pixel immagine\n",
        "#             'bbox': (x0, y0, x1, y1)                            # bbox usata\n",
        "#         }\n",
        "#     \"\"\"\n",
        "#     # --- 1) Sanity & conversioni ---\n",
        "#     q_pil = ensure_pil(query_image)  if not isinstance(query_image, str) else ensure_pil(load_rgb_image(query_image))\n",
        "#     t_pil = ensure_pil(test_image) if not isinstance(test_image, str) else ensure_pil(load_rgb_image(test_image))\n",
        "\n",
        "#     x0, y0, x1, y1 = map(int, bbox)\n",
        "#     Wq, Hq = q_pil.size\n",
        "#     print(\"Shape query image: \", Wq, Hq)\n",
        "#     x0 = max(0, min(x0, Wq - 1))\n",
        "#     x1 = max(0, min(x1, Wq - 1))\n",
        "#     y0 = max(0, min(y0, Hq - 1))\n",
        "#     y1 = max(0, min(y1, Hq - 1))\n",
        "#     if x1 <= x0 or y1 <= y0:\n",
        "#         print(x0,x1, \" \", y0,y1)\n",
        "#         raise ValueError(f\"BBox non valida: {bbox}\")\n",
        "\n",
        "#     crop_pil = q_pil.crop((x0, y0, x1, y1))\n",
        "\n",
        "#     # --- 2) Carica backbone & trasformazioni ---\n",
        "#     forward_fn, transform_fn, postproc_fn, label = get_backbone_and_transform(\n",
        "#         model=model,\n",
        "#         model_variant=model_variant,\n",
        "#         sam_checkpoint=sam_checkpoint,\n",
        "#         dinov3_hf_vit_repo=dinov3_hf_vit_repo,\n",
        "#     )\n",
        "\n",
        "#     # --- 3) Estrai feature (C,H,W) ---\n",
        "#     templ_feat = extract_spatial_features_from_image(crop_pil, forward_fn, transform_fn, postproc_fn)  # (C,hk,wk)\n",
        "#     test_feat  = extract_spatial_features_from_image(t_pil,  forward_fn, transform_fn, postproc_fn)   # (C,ht,wt)\n",
        "\n",
        "#     # (opzionale) ridimensiona il template in feature-space per robustezza/velocità\n",
        "#     if template_target_hw is not None:\n",
        "#         Htgt, Wtgt = template_target_hw\n",
        "#         Htgt = max(3, int(Htgt))\n",
        "#         Wtgt = max(3, int(Wtgt))\n",
        "#         templ_feat = resize_features_chw(templ_feat, (Htgt, Wtgt))\n",
        "\n",
        "#     # --- 4) Cosine template matching denso nell spazio feature ---\n",
        "#     sim_map = cosine_template_match(test_feat, templ_feat, stride=stride)  # (H', W')\n",
        "\n",
        "#     # --- 5) Prendi il picco e rimappalo in coordinate immagine per comodità ---\n",
        "#     y_f, x_f = np.unravel_index(sim_map.argmax(), sim_map.shape)  # feature-space (output dell'unfold)\n",
        "#     # upsample grezza della sim_map alla risoluzione immagine (così otteniamo un (x,y) in pixel)\n",
        "#     sm = sim_map.astype(np.float32)\n",
        "#     sm_min, sm_max = float(sm.min()), float(sm.max())\n",
        "#     if sm_max == sm_min:\n",
        "#         sm_norm = np.zeros_like(sm, dtype=np.uint8)\n",
        "#     else:\n",
        "#         sm_norm = ((sm - sm_min) / (sm_max - sm_min) * 255.0).clip(0, 255).astype(np.uint8)\n",
        "#     heat = Image.fromarray(sm_norm).resize(t_pil.size, resample=Image.BICUBIC)  # (Wimg, Himg)\n",
        "#     heat_np = np.array(heat)\n",
        "#     y_img, x_img = np.unravel_index(heat_np.argmax(), heat_np.shape)  # (Himg, Wimg)\n",
        "\n",
        "#     # --- 6) Visualizza opzionalmente ---\n",
        "#     if visualize:\n",
        "#         visualize_similarity_heatmap(t_pil, sim_map, alpha=0.65,\n",
        "#                                      title=f\"Cosine Similarity — {label}\")\n",
        "\n",
        "#         # marca il picco sulla test image\n",
        "#         plt.figure(figsize=(6, 6))\n",
        "#         plt.imshow(t_pil)\n",
        "#         plt.scatter([x_img], [y_img], s=80, c='red', marker='+')\n",
        "#         plt.title(f\"Peak @ ({x_img}, {y_img}) | cos={sim_map.max():.3f}\")\n",
        "#         plt.axis(\"off\")\n",
        "#         plt.show()\n",
        "\n",
        "#     return {\n",
        "#         \"sim_map\": sim_map,\n",
        "#         \"label\": label,\n",
        "#         \"query_feat_shape\": tuple(templ_feat.shape),\n",
        "#         \"test_feat_shape\": tuple(test_feat.shape),\n",
        "#         \"peak_feature_xy\": (int(y_f), int(x_f)),\n",
        "#         \"peak_image_xy\": (int(y_img), int(x_img)),\n",
        "#         \"bbox\": (x0, y0, x1, y1),\n",
        "#     }\n",
        "\n",
        "\n",
        "# def features_cosine_map_from_patch(features_chw: np.ndarray, py: int, px: int) -> np.ndarray:\n",
        "#     \"\"\"\n",
        "#     Calcola la mappa di cosine similarity tra il patch (py, px) e tutti\n",
        "#     gli altri patch nello stesso feature map (C,H,W).\n",
        "#     Ritorna una mappa (H, W) con valori in [-1, 1].\n",
        "#     \"\"\"\n",
        "#     if features_chw.ndim != 3:\n",
        "#         raise ValueError(f\"Expected (C,H,W), got {features_chw.shape}\")\n",
        "#     C, H, W = features_chw.shape\n",
        "#     if not (0 <= py < H and 0 <= px < W):\n",
        "#         raise ValueError(f\"Patch index out of range: py={py}, px={px}, H={H}, W={W}\")\n",
        "\n",
        "#     f = torch.from_numpy(features_chw).float()      # (C,H,W)\n",
        "#     f = F.normalize(f, dim=0)                       # L2-normalize lungo C per ogni loc\n",
        "\n",
        "#     q = f[:, py, px]                                # (C,)\n",
        "#     q = F.normalize(q, dim=0)                       # (C,)\n",
        "\n",
        "#     # cosine = dot(q, f[:,y,x]) per tutti (y,x) -> equivalente a (q^T @ f) lungo C\n",
        "#     sim = (q.view(C, 1, 1) * f).sum(dim=0)          # (H, W)\n",
        "#     return sim.cpu().numpy()\n",
        "\n",
        "# def image_to_feature_coords(img_hw: tuple[int,int], feat_hw: tuple[int,int], y_img: int, x_img: int) -> tuple[int,int]:\n",
        "#     \"\"\"\n",
        "#     Converte coordinate immagine (pixel) in coordinate feature (patch).\n",
        "#     Usa scalatura nearest neighbor.\n",
        "#     \"\"\"\n",
        "#     H_img, W_img = img_hw\n",
        "#     Hf, Wf = feat_hw\n",
        "#     py = int(round((y_img / max(1, H_img - 1)) * (Hf - 1)))\n",
        "#     px = int(round((x_img / max(1, W_img - 1)) * (Wf - 1)))\n",
        "#     py = max(0, min(Hf - 1, py))\n",
        "#     px = max(0, min(Wf - 1, px))\n",
        "#     return py, px\n",
        "\n",
        "# def visualize_patch_similarity(test_img: Image.Image,\n",
        "#                                sim_map: np.ndarray,\n",
        "#                                y_img: int,\n",
        "#                                x_img: int,\n",
        "#                                alpha: float = 0.65,\n",
        "#                                title: str = \"DINOv3: Cosine map from red-cross patch\"):\n",
        "#     \"\"\"\n",
        "#     Mostra la mappa di similarità (ridimensionata all'immagine) e la croce rossa\n",
        "#     nel punto sorgente (y_img, x_img).\n",
        "#     \"\"\"\n",
        "#     if sim_map.ndim != 2:\n",
        "#         raise ValueError(f\"sim_map must be (H,W), got {sim_map.shape}\")\n",
        "\n",
        "#     H_img, W_img = test_img.size[1], test_img.size[0]\n",
        "#     sm = sim_map.astype(np.float32)\n",
        "#     sm_min, sm_max = float(sm.min()), float(sm.max())\n",
        "#     if sm_max == sm_min:\n",
        "#         sm_n = np.zeros_like(sm, dtype=np.uint8)\n",
        "#     else:\n",
        "#         sm_n = ((sm - sm_min) / (sm_max - sm_min) * 255.0).clip(0, 255).astype(np.uint8)\n",
        "#     heat = Image.fromarray(sm_n).resize((W_img, H_img), resample=Image.BICUBIC)\n",
        "\n",
        "#     plt.figure(figsize=(12, 6))\n",
        "#     plt.subplot(1, 2, 1)\n",
        "#     plt.imshow(test_img)\n",
        "#     plt.scatter([x_img], [y_img], s=60, c='red', marker='+')  # croce rossa\n",
        "#     plt.title(\"Red-cross patch (reference)\")\n",
        "#     plt.axis(\"off\")\n",
        "\n",
        "#     plt.subplot(1, 2, 2)\n",
        "#     plt.imshow(test_img)\n",
        "#     plt.imshow(heat, cmap=\"viridis\", alpha=alpha)\n",
        "#     plt.scatter([x_img], [y_img], s=60, c='red', marker='+')\n",
        "#     plt.title(title)\n",
        "#     plt.axis(\"off\")\n",
        "#     plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1) Scegli DINOv3 (o altra backbone che dia (C,H,W))\n",
        "# model_name = \"dinov3\"\n",
        "# model_variant = \"vit_base_patch16_224.dino\"   # es. timm\n",
        "# forward_fn, transform_fn, postproc_fn, label = get_backbone_and_transform(\n",
        "#     model=model_name,\n",
        "#     model_variant=model_variant,\n",
        "#     dinov3_hf_vit_repo=None,     # opzionale fallback HF\n",
        "# )\n",
        "\n",
        "# # 2) Assicurati che test_image sia PIL\n",
        "# test_pil = ensure_pil(load_rgb_image(test_image))\n",
        "\n",
        "# # 3) Estrai feature spaziali della test image\n",
        "# test_feat = extract_spatial_features_from_image(test_pil, forward_fn, transform_fn, postproc_fn)  # (C,Hf,Wf)\n",
        "# C, Hf, Wf = test_feat.shape\n",
        "# print(f\"{label} features: {test_feat.shape}\")\n",
        "\n",
        "# # 4) Scegli la posizione della croce rossa in coordinate IMM (pixel)\n",
        "# #    (Puoi prenderle da un click interattivo o fissarle manualmente)\n",
        "\n",
        "# query_image_pre_bbox = load_rgb_image(query_image_pre_bbox)\n",
        "\n",
        "# x1,y1, x2, y2 = current_box\n",
        "# y_img, x_img = (y1+y2)//2, (x1+x2)//2\n",
        "# # y_img, x_img = 200, 300   # <-- rimpiazza con le tue coordinate immagine\n",
        "\n",
        "\n",
        "# # 5) Converti coordinate immagine -> coordinate feature (patch indices)\n",
        "# py, px = image_to_feature_coords((query_image_pre_bbox.size[1], query_image_pre_bbox.size[0]), (Hf, Wf), y_img, x_img)\n",
        "# print(f\"Red-cross patch @ feature coords: (py={py}, px={px}) / (Hf={Hf}, Wf={Wf})\")\n",
        "\n",
        "# # 6) Calcola la cosine similarity tra il patch selezionato e tutti gli altri patch\n",
        "# sim_map = features_cosine_map_from_patch(test_feat, py=py, px=px)     # (Hf, Wf)\n",
        "# print(f\"sim_map: shape={sim_map.shape}, min={sim_map.min():.3f}, max={sim_map.max():.3f}\")\n",
        "\n",
        "# # 7) Visualizza: croce rossa + heatmap\n",
        "# visualize_patch_similarity(test_pil, sim_map, y_img=y_img, x_img=x_img,\n",
        "#                            alpha=0.65,\n",
        "#                            title=f\"Cosine Similarity from red-cross — {label}\")\n",
        "\n",
        "\n",
        "query_image = ensure_pil(query_image)  if not isinstance(query_image, str) else ensure_pil(load_rgb_image(query_image))\n",
        "test_image = ensure_pil(test_image) if not isinstance(test_image, str) else ensure_pil(load_rgb_image(test_image))\n",
        "\n",
        "\n",
        "\n",
        "res = cosine_similarity_from_bbox(\n",
        "    query_image=query_image_pre_bbox,   # immagine sorgente (non croppata)\n",
        "    bbox=current_box,                   # (x0,y0,x1,y1) in pixel sulla query\n",
        "    test_image=test_image,              # immagine target\n",
        "    model=\"dinov3\",\n",
        "    model_variant=\"vit_base_patch16_224.dino\",\n",
        "    template_target_hw=(9, 9),\n",
        "    stride=1,\n",
        "    visualize=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "current_box"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scales = [0.75, 1.0, 1.25]\n",
        "best = None\n",
        "for s in scales:\n",
        "    Hs = max(3, int(round(templ_feat.shape[1] * s)))\n",
        "    Ws = max(3, int(round(templ_feat.shape[2] * s)))\n",
        "    tf = resize_features_chw(query_feat, (Hs, Ws))\n",
        "    sm = cosine_template_match(test_feat, tf, stride=1)\n",
        "    m = sm.max()\n",
        "    if (best is None) or (m > best[0]):\n",
        "        best = (m, sm, s)\n",
        "\n",
        "print(f\"Best scale={best[2]} | best max cos={best[0]:.3f}\")\n",
        "visualize_similarity_heatmap(test_pil, best[1], alpha=0.65,\n",
        "                             title=f\"Cosine Similarity (Best scale={best[2]}) — {label}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x1,y1, x2, y2 = current_box\n",
        "y_img, x_img = (y1+y2)//2, (x1+x2)//2\n",
        "\n",
        "print(current_box)\n",
        "print(y_img, x_img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
